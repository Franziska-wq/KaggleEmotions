{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - Find average number of negations in each category\n",
    "**For this, I loop in each category of tweets, and test if there are negation words or prefixes or suffixes inside. I add this to a variable named ```negation_count``` that i divide by the number of tweets at the end. I created a third column that i add to the dataset**  \n",
    "\n",
    "Response time : 10s  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "negation_words = {'no', 'not', 'neither', 'never', 'no one', 'nobody', 'none', 'nor', 'nothing', 'nowhere'}\n",
    "prefixes = {'un', 'im', 'in', 'il', 'ir', 'dis'}\n",
    "suffix = 'less'\n",
    "\n",
    "def find_average_number_of_negation_in_each_category(file_path, file_name):\n",
    "    average_number_of_negation_in_each_category = []\n",
    "    df = pd.read_csv(\"{}/{}.csv\".format(file_path, file_name))\n",
    "    for tweets in df[\"Concatenated_Tweets\"]:\n",
    "        negation_count = 0\n",
    "        separated_tweets = tweets.replace(\"[\", \"\").replace(\"]\", \"\").split(',')\n",
    "        for tweet in separated_tweets:\n",
    "            tokens = nltk.word_tokenize(tweet)\n",
    "            negation_count += sum(1 for token in tokens if token in negation_words)\n",
    "            negation_count += sum(1 for token in tokens if any(token.startswith(prefix) for prefix in prefixes))\n",
    "            negation_count += sum(1 for token in tokens if token.endswith(suffix))\n",
    "        average_number_of_negation_in_each_category.append(round(negation_count/len(separated_tweets), 3))\n",
    "    df[\"Average_Number_Of_Negation\"]=average_number_of_negation_in_each_category\n",
    "    return df\n",
    "            \n",
    "    \n",
    "\n",
    "test_with_negation = find_average_number_of_negation_in_each_category('categories','test')\n",
    "train_with_negation = find_average_number_of_negation_in_each_category('categories','train')\n",
    "val_with_negation = find_average_number_of_negation_in_each_category('categories','val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 - Training a SVM classifier (with n-gram TF-IDF vecotrization)\n",
    "**I vectorized the features of the train set and the test set and created the model ```svm_classifier``` and fitted it with train set. Then, I predicted the results with the test set and computed all f1-scores and displayed them by categories**  \n",
    "\n",
    "Response time : 7m  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/franzi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported\n",
      "Vectorization done\n",
      "Blank model created\n",
      "Model trained\n",
      "Prediction done\n",
      "F1-score for anger: 0.5505376344086022\n",
      "F1-score for fear: 0.5891472868217054\n",
      "F1-score for joy: 0.7242848447961048\n",
      "F1-score for love: 0.28571428571428575\n",
      "F1-score for sadness: 0.7054963084495488\n",
      "F1-score for surprise: 0.23684210526315788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# import the python file\n",
    "import functions_and_variables as fs\n",
    "\n",
    "def read_txt_data_and_split(file_path, file_name, should_preprocess=False):\n",
    "    df = pd.read_csv(\"{}/{}.txt\".format(file_path, file_name), sep=\";\", header=None)\n",
    "    df.columns=[\"Tweet\", \"Emotion\"]\n",
    "    \n",
    "    # preprocess data if the optionally passed parameter is set to True\n",
    "    if should_preprocess:\n",
    "        for i, tweet in df.iterrows():\n",
    "            df.at[i, \"Tweet\"] = \" \".join(fs.preprocess(df.at[i,\"Tweet\"], True)[0])\n",
    "    return df[\"Tweet\"], df[\"Emotion\"]\n",
    "\n",
    "X_train, y_train = read_txt_data_and_split(\"data\", \"train\", True)\n",
    "X_test, y_test = read_txt_data_and_split(\"data\", \"test\", True)\n",
    "print(\"Data imported\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,3), analyzer='char', max_features=500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "print(\"Vectorization done\")\n",
    "\n",
    "svm_classifier = SVC()\n",
    "print(\"Blank model created\")\n",
    "\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "print(\"Model trained\")\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "print(\"Prediction done\")\n",
    "\n",
    "f1_svm = f1_score(y_test, y_pred, average=None)\n",
    "category_names = svm_classifier.classes_\n",
    "\n",
    "for category, score in zip(category_names, f1_svm):\n",
    "    print(f\"F1-score for {category}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8 - Training a classifier (with bag-of-word TF-IDF vecotrization)\n",
    "**I vectorized the features of the train set and the test set and created the model ```random_forest``` and fitted it with train set. Then, I predicted the results with the test set and computed all f1-scores and displayed them by categories**  \n",
    "\n",
    "Response time : 1m  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported\n",
      "Vectorization done\n",
      "Blank model created\n",
      "Model trained\n",
      "Prediction done\n",
      "F1-score for anger: 0.39999999999999997\n",
      "F1-score for fear: 0.5155807365439095\n",
      "F1-score for joy: 0.635593220338983\n",
      "F1-score for love: 0.3228699551569507\n",
      "F1-score for sadness: 0.5606425702811244\n",
      "F1-score for surprise: 0.358974358974359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, y_train = read_txt_data_and_split(\"data\", \"train\")\n",
    "X_test, y_test = read_txt_data_and_split(\"data\", \"test\")\n",
    "print(\"Data imported\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "print(\"Vectorization done\")\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "print(\"Blank model created\")\n",
    "\n",
    "random_forest.fit(X_train_tfidf, y_train)\n",
    "print(\"Model trained\")\n",
    "\n",
    "y_pred = random_forest.predict(X_test_tfidf)\n",
    "print(\"Prediction done\")\n",
    "\n",
    "f1_forest = f1_score(y_test, y_pred, average=None)\n",
    "category_names = random_forest.classes_\n",
    "\n",
    "for category, score in zip(category_names, f1_forest):\n",
    "    print(f\"F1-score for {category}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's try ```SVC()``` again with the bag-of-word method and compare the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blank model created\n",
      "Model trained\n",
      "Prediction done\n",
      "F1-score for anger: 0.5505376344086022, 0.3723404255319148\n",
      "F1-score for fear: 0.5891472868217054, 0.5098039215686275\n",
      "F1-score for joy: 0.7242848447961048, 0.6522001205545509\n",
      "F1-score for love: 0.28571428571428575, 0.2898550724637681\n",
      "F1-score for sadness: 0.7054963084495488, 0.5875190258751902\n",
      "F1-score for surprise: 0.23684210526315788, 0.29885057471264365\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = SVC()\n",
    "print(\"Blank model created\")\n",
    "\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "print(\"Model trained\")\n",
    "\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "print(\"Prediction done\")\n",
    "\n",
    "f1_svm_2 = f1_score(y_test, y_pred, average=None)\n",
    "category_names = svm_classifier.classes_\n",
    "\n",
    "for category, score_svm_1, score_svm_2 in zip(category_names, f1_svm, f1_svm_2):\n",
    "    print(f\"F1-score for {category}: {score_svm_1}, {score_svm_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "26a650b9244880cda9b3c29eedb27f415fa4f6f8549b7618a68bcac8b230ec01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
