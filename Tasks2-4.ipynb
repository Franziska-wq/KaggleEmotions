{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bed38b",
   "metadata": {},
   "source": [
    "# General\n",
    "\n",
    "Relevant analysis in those tasks were done with preprocessed data as well. Those files have the extension '\\_preprossed at' the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fee3c3",
   "metadata": {},
   "source": [
    "# Exc 2\n",
    "\n",
    "**In this exercise we first created datafiles for future use. The data is stored in the form of DataFrames.**\n",
    "**For the next step we performed the LDA Analysis to find 3 topics with 4 keywords for each category.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018d8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports for exercise 2\n",
    "\n",
    "import functions_and_variables as fs # file with useful functions and a few variables\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "#nltk.download('words')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfcd52",
   "metadata": {},
   "source": [
    "### Create csv files \n",
    "\n",
    "The data is read from the text files and stored in the typical DataFrame form {category: [tweets]}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d3e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes [key: sentences] for test were successfully created and stored as .csv\n",
      "Dataframes [key: sentences] for val were successfully created and stored as .csv\n",
      "Dataframes [key: sentences] for train were successfully created and stored as .csv\n",
      "Dataframes [key: sentences] for complete were successfully created and stored as .csv\n"
     ]
    }
   ],
   "source": [
    "# function to read and concatenate a file and save it as dataframe into a .csv\n",
    "def concat_categories_to_file(file_path, file_name, export_path):\n",
    "    data = pd.read_csv(\"{}/{}.txt\".format(file_path,file_name),sep=';')\n",
    "    tweets = data.iloc[:, 0]\n",
    "    categories = data.iloc[:, 1]\n",
    "    \n",
    "    category_tweets = {}\n",
    "    for category, tweet in zip(categories, tweets):\n",
    "        if category not in category_tweets:\n",
    "            category_tweets[category] = []\n",
    "        category_tweets[category].append(tweet)\n",
    "\n",
    "    result = pd.DataFrame(category_tweets.items(),columns=['Category', 'Concatenated_Tweets'])\n",
    "    result.to_csv('{}/{}.csv'.format(export_path,file_name), index=False, encoding='utf-8')\n",
    "    print('Dataframes [key: sentences] for {} were successfully created and stored as .csv'.format(file_name))\n",
    "    return category_tweets\n",
    "\n",
    "for file in ['test','val','train','complete']:\n",
    "    if file == 'complete':\n",
    "        dataframe_categories = concat_categories_to_file('data',file,'categories')\n",
    "    else:\n",
    "        concat_categories_to_file('data',file,'categories')\n",
    "\n",
    "dataframe_categories = dict(sorted(dataframe_categories.items()))\n",
    "categories = dataframe_categories.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a92c0e",
   "metadata": {},
   "source": [
    "### Perform the Latent Dirichlet Allocation\n",
    "\n",
    "The data in this code is not preprocessed but we have run the analysis with preprocessed data as well, so there are files in the same foler with '\\_preprocessed' at the end. That can be done by setting _shoud_preprocess=True_. \n",
    "\n",
    "Inspired from\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81330511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger:\n",
      "[(0, '0.083*\"i\" + 0.032*\"and\" + 0.029*\"feel\" + 0.024*\"the\"'),\n",
      " (1, '0.070*\"i\" + 0.034*\"the\" + 0.032*\"to\" + 0.026*\"feel\"'),\n",
      " (2, '0.083*\"i\" + 0.040*\"feel\" + 0.026*\"to\" + 0.026*\"and\"')]\n",
      "File for \"anger\" successfully created at: ./results/lda/lda_ana_anger.csv\n",
      "fear:\n",
      "[(0, '0.085*\"i\" + 0.035*\"to\" + 0.034*\"and\" + 0.030*\"feel\"'),\n",
      " (1, '0.058*\"i\" + 0.026*\"the\" + 0.025*\"a\" + 0.024*\"feel\"'),\n",
      " (2, '0.071*\"i\" + 0.034*\"feel\" + 0.026*\"the\" + 0.024*\"feeling\"')]\n",
      "File for \"fear\" successfully created at: ./results/lda/lda_ana_fear.csv\n",
      "joy:\n",
      "[(0, '0.081*\"i\" + 0.040*\"feel\" + 0.035*\"the\" + 0.032*\"to\"'),\n",
      " (1, '0.066*\"i\" + 0.034*\"feel\" + 0.032*\"and\" + 0.028*\"to\"'),\n",
      " (2, '0.075*\"i\" + 0.028*\"and\" + 0.026*\"feel\" + 0.024*\"to\"')]\n",
      "File for \"joy\" successfully created at: ./results/lda/lda_ana_joy.csv\n",
      "love:\n",
      "[(0, '0.055*\"i\" + 0.034*\"and\" + 0.024*\"feel\" + 0.021*\"to\"'),\n",
      " (1, '0.072*\"i\" + 0.034*\"the\" + 0.033*\"and\" + 0.027*\"feel\"'),\n",
      " (2, '0.071*\"i\" + 0.040*\"feel\" + 0.031*\"to\" + 0.021*\"and\"')]\n",
      "File for \"love\" successfully created at: ./results/lda/lda_ana_love.csv\n",
      "sadness:\n",
      "[(0, '0.080*\"i\" + 0.040*\"feel\" + 0.033*\"and\" + 0.028*\"to\"'),\n",
      " (1, '0.082*\"i\" + 0.032*\"and\" + 0.032*\"feel\" + 0.024*\"to\"'),\n",
      " (2, '0.087*\"i\" + 0.029*\"feel\" + 0.024*\"the\" + 0.022*\"to\"')]\n",
      "File for \"sadness\" successfully created at: ./results/lda/lda_ana_sadness.csv\n",
      "surprise:\n",
      "[(0, '0.077*\"i\" + 0.028*\"feel\" + 0.026*\"and\" + 0.023*\"to\"'),\n",
      " (1, '0.059*\"i\" + 0.039*\"the\" + 0.034*\"and\" + 0.022*\"feel\"'),\n",
      " (2, '0.072*\"i\" + 0.032*\"feel\" + 0.024*\"to\" + 0.021*\"and\"')]\n",
      "File for \"surprise\" successfully created at: ./results/lda/lda_ana_surprise.csv\n"
     ]
    }
   ],
   "source": [
    "def perform_lda(category: str, data: list, shoud_preprocess: bool, num_topis: int, num_words: int):\n",
    "    \n",
    "    data_words = []\n",
    "    path_preprocessed = ''\n",
    "\n",
    "    if not shoud_preprocess:\n",
    "        data_words = [word_tokenize(sentence) for sentence in data]\n",
    "\n",
    "    else:\n",
    "        data_words.append(fs.preprocess(data))\n",
    "        path_preprocessed = '_preprocessed'\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    \n",
    "    # Create Corpus\n",
    "    texts = data_words\n",
    "    \n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=id2word,\n",
    "                                           num_topics=num_topics)\n",
    "\n",
    "    # Print the Keyword with the specified number of words\n",
    "    print(category + \":\")\n",
    "    keywords = lda_model.print_topics(num_words=num_words)\n",
    "    pprint(keywords)\n",
    "    file_path = fs.result_path + 'lda/lda_ana_' + category + path_preprocessed + '.csv'\n",
    "    fs.write_to_file(file_path=file_path, \n",
    "                     content=str(keywords), new=True)\n",
    "    print('File for \"' + category +'\" successfully created at: ' + file_path)\n",
    "    return id2word, corpus, lda_model\n",
    "\n",
    "num_topics = 3\n",
    "num_words = 4\n",
    "lda_dict = dict.fromkeys(dataframe_categories.keys(), [])\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "index = 0\n",
    "for category in categories:    \n",
    "    # Perform LDA for the current topic\n",
    "    lda_dict[category] = perform_lda(category=category, \n",
    "                                             data=dataframe_categories[category], shoud_preprocess=False, \n",
    "                                             num_topis=num_topics, num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b055d",
   "metadata": {},
   "source": [
    "### Extra function and creation of new preprocessed files\n",
    "\n",
    "Those .csv files were created for future use and store data in the same way as mentioned above. However, the tweets are prepocessed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba4bc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for \"test\" successfully created at: categories\n",
      "File for \"val\" successfully created at: categories\n",
      "File for \"train\" successfully created at: categories\n",
      "File for \"complete\" successfully created at: categories\n"
     ]
    }
   ],
   "source": [
    "def write_preprocessed_csv(file_path, file_name, export_path):\n",
    "    file = fs.read_file('{}/{}.txt'.format(file_path,file_name),';')\n",
    "    cleaned_dataframe = {key:[] for key in categories}\n",
    "    for line in file:\n",
    "        cleaned_dataframe[line[1]].append(' '.join(fs.preprocess(line[0], False)))\n",
    "\n",
    "    result = pd.DataFrame(cleaned_dataframe.items(),columns=['Category', 'Concatenated_Tweets'])\n",
    "    result.to_csv('{}/{}_preprocessed.csv'.format(export_path,file_name), index=False, encoding='utf-8')\n",
    "    print('File for \"' + file_name +'\" successfully created at: ' + export_path)\n",
    "    \n",
    "for file in ['test','val','train','complete']:\n",
    "    write_preprocessed_csv('data',file,'categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5bb40",
   "metadata": {},
   "source": [
    "# Exc 3\n",
    "\n",
    "Inspired from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0, we created a visualization using **pyLDAvis** to understand the intertopic relationsships as well as interpret the individual topics. These can either be opended from the folder, or directly via the notebook by removing the comments mentioned below.\n",
    "However, we also calculated the **intertopic distances** by using the amount of **common keywords** as well as the **cosine similarity** of the corresponding embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b5cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "from scipy.spatial import distance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfcc9cf",
   "metadata": {},
   "source": [
    "## pyLDAvis\n",
    "\n",
    "For more information refer to the chapter **Analyzing LDA model results** at the above link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d495223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for \"anger\" successfully created at: ./results/lda/model/ldavis_prepared_anger_3.html\n",
      "File for \"fear\" successfully created at: ./results/lda/model/ldavis_prepared_fear_3.html\n",
      "File for \"joy\" successfully created at: ./results/lda/model/ldavis_prepared_joy_3.html\n",
      "File for \"love\" successfully created at: ./results/lda/model/ldavis_prepared_love_3.html\n",
      "File for \"sadness\" successfully created at: ./results/lda/model/ldavis_prepared_sadness_3.html\n",
      "File for \"surprise\" successfully created at: ./results/lda/model/ldavis_prepared_surprise_3.html\n"
     ]
    }
   ],
   "source": [
    "# Use pyLDAvis to visualize LDA\n",
    "def visualize_lda(category: str, id2word, corpus, lda_model, num_topics: int):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(fs.result_path + 'lda/model/', exist_ok=True)\n",
    "    \n",
    "    filename = fs.result_path + 'lda/model/ldavis_prepared_' + category + '_' +  str(num_topics)\n",
    "    LDAvis_data_filepath = os.path.join(filename)\n",
    "    \n",
    "    ### this is a bit time consuming - make the if statement True\n",
    "    ### if you want to execute visualization prep yourself\n",
    "    if 1 == 1:\n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, n_jobs=1)\n",
    "        \n",
    "        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "            pickle.dump(LDAvis_prepared, f)\n",
    "    \n",
    "    # load the pre-prepared pyLDAvis data from disk\n",
    "    with open(LDAvis_data_filepath, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "    html_filename = filename + '.html'\n",
    "    pyLDAvis.save_html(LDAvis_prepared, html_filename)\n",
    "    print('File for \"' + category + '\" successfully created at: ' + html_filename)\n",
    "    os. remove(LDAvis_data_filepath)\n",
    "    return LDAvis_prepared, html_filename\n",
    "\n",
    "for category in categories:    \n",
    "    # Create visualisation\n",
    "    prepared_data, html_filename = visualize_lda(category=category, id2word=lda_dict[category][0], \n",
    "                                                 corpus=lda_dict[category][1], \n",
    "                                                 lda_model=lda_dict[category][2], \n",
    "                                                 num_topics=num_topics)\n",
    "    \n",
    "    # Display the visualization directly in the notebook\n",
    "    #display(pyLDAvis.display(prepared_data))\n",
    "    \n",
    "    # Open the HTML files in a web browser (if running on windows: replace \"open \" with \"start \")\n",
    "    #os.system(\"open \" + html_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246fb513",
   "metadata": {},
   "source": [
    "## Intertopic distances \n",
    "\n",
    "For **I1** (common keywords): The algorithm currently returns the total **amount**. This can easily be changed to a relative measure by removing a comment in the code below.\n",
    "\n",
    "For **I2** (semantic similarity): The embedding vectors are the **average word2vec embeddings** of the four keywords of a topic. The embedding vectors are then used to claculate the semantic similarity (using **cosine similarity**) between all topic pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef8533f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display:flex\"><div style=\"margin-right: 32px\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>topics</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>max_values</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style=\"margin-right: 32px\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>topics</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fear</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fear</td>\n",
       "      <td>max_values</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style=\"margin-right: 32px\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>topics</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>joy</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>joy</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>joy</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>joy</td>\n",
       "      <td>max_values</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display:flex\"><div style=\"margin-right: 32px\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>topics</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>love</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>love</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>love</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>love</td>\n",
       "      <td>max_values</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style=\"margin-right: 32px\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>topics</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sadness</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sadness</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sadness</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.970296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sadness</td>\n",
       "      <td>max_values</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style=\"margin-right: 32px\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>topics</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>surprise</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>surprise</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>surprise</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>surprise</td>\n",
       "      <td>max_values</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean keywords by extracing special characters and numbers\n",
    "def clean_keywords(keywords):\n",
    "    words = []\n",
    "    for word in keywords:\n",
    "        word = ''.join(char for char in word if char.isalpha())\n",
    "        words.append(str(word))\n",
    "    return words\n",
    "\n",
    "# calculate the total amount of common keywords (measure1) and the cosine similarity(measure2)\n",
    "def calculate_closeness(topic1, topic2, model, vector_size):\n",
    "    topic1_words = clean_keywords(topic1.split('+'))\n",
    "    topic2_words = clean_keywords(topic2.split('+'))\n",
    "    \n",
    "    #### I1 - count common keywords of topics ####\n",
    "    i1 = 0;\n",
    "    for word in topic1_words:\n",
    "        if word in topic2_words: \n",
    "            i1 += 2\n",
    "    \n",
    "    # transform total to relative measure\n",
    "    #measure1 = points/0.5*len(topic1 + topic2) # get percentage\n",
    "    \n",
    "    #### I2 - semantic similarity ####\n",
    "    # calculate avg. word2vec embeddings per topic\n",
    "    avg_topic_vector1 = np.empty(vector_size)\n",
    "    for word in topic1_words:\n",
    "        avg_topic_vector1 += model.wv[word]\n",
    "    avg_topic_vector1 /= len(topic1_words)\n",
    "    \n",
    "    avg_topic_vector2 = np.empty(vector_size)\n",
    "    for word in topic2_words:\n",
    "        avg_topic_vector2 += model.wv[word]\n",
    "    avg_topic_vector2 /= len(topic2_words)\n",
    "    \n",
    "    # calculate cosine similarity\n",
    "    i2 = 1 - distance.cosine(avg_topic_vector1, avg_topic_vector2)\n",
    "    \n",
    "    return i1, i2\n",
    "\n",
    "# function to display DataFrames horizontally, require a list of DataFrames\n",
    "def horizontal(dfs):\n",
    "    html = '<div style=\"display:flex\">'\n",
    "    for df in dfs:\n",
    "        html += '<div style=\"margin-right: 32px\">'\n",
    "        html += df.to_html()\n",
    "        html += '</div>'\n",
    "    html += '</div>'\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "df_list = [] # List of DataFrames for each category\n",
    "index = 0 # index to iterate through dataframe\n",
    "vector_size = 1000 # vector size for model\n",
    "\n",
    "# loop through the categories and calculate pairwise comparison of topics\n",
    "for category in categories:\n",
    "    closeness_data = pd.DataFrame(columns=['category','topics','I1','I2']) # dataframe to store values in\n",
    "\n",
    "    # get lda_resut for the current category and transfer it to tuples\n",
    "    with open('./results/lda/lda_ana_' + category + '.csv', 'r') as file:\n",
    "        lda_data = file.readline()\n",
    "    topics = ast.literal_eval(lda_data)\n",
    "    \n",
    "    # get training data for model (dependent on categorie) \n",
    "    # and transform it to a list of words per sentence\n",
    "    model_data = []    \n",
    "    for sentence in dataframe_categories[category]:\n",
    "        model_data.append(sentence.split(' '))\n",
    "        \n",
    "    # train model\n",
    "    model = gensim.models.Word2Vec(sentences=model_data, min_count=1, \n",
    "                              vector_size=vector_size, window=5)\n",
    "            \n",
    "    # calculate closeness of topics and identify highest values\n",
    "    max_i1 = 0\n",
    "    max_i2 = 0\n",
    "    for i in range(len(topics)):\n",
    "        for j in range(i + 1, len(topics)):\n",
    "            closeness = calculate_closeness(topic1=topics[i][1], topic2=topics[j][1], \n",
    "                                            model=model, vector_size=vector_size)\n",
    "            closeness_data.loc[index] = category, f\"T{i}&T{j}\", closeness[0], closeness[1]\n",
    "            index += 1\n",
    "            max_i1 = closeness[0] if closeness[0] > max_i1 else max_i1\n",
    "            max_i2 = closeness[1] if closeness[1] > max_i2 else max_i2\n",
    "        closeness_data.loc[index] = category, \"max_values\", max_i1, max_i2\n",
    "    \n",
    "    # add DataFrame with the topic data to the list\n",
    "    df_list.append(closeness_data)\n",
    "    \n",
    "    # print as many dataframes in a row as there are topics\n",
    "    list_index = int(index/len(topics)) - 1\n",
    "    if (index) % (len(topics)**2) == 0:\n",
    "        horizontal(df_list[list_index- (len(topics) - 1):list_index+1])\n",
    "    elif index + 2 > len(topics) * len(categories) - 1:\n",
    "        horizontal(df_list[list_index:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e706bc",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "In this Exercise we calculated the **linguistic quality ratio** for the given text. As the data from Kaggle was already preprocessed, we expected and got 0 values for symbols, links etc. However, as the input data are tweets, the quality is pretty **low**. There are many stopwords and words that are not in wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65e213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total tokens</th>\n",
       "      <th>Linguistic Quality Ratio</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>not in wordnet</th>\n",
       "      <th>symbols/numbers/etc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>52177</td>\n",
       "      <td>0.390057</td>\n",
       "      <td>26879</td>\n",
       "      <td>4946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>44498</td>\n",
       "      <td>0.398782</td>\n",
       "      <td>22763</td>\n",
       "      <td>3990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>131401</td>\n",
       "      <td>0.401565</td>\n",
       "      <td>67405</td>\n",
       "      <td>11230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>33833</td>\n",
       "      <td>0.389590</td>\n",
       "      <td>17521</td>\n",
       "      <td>3131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>106784</td>\n",
       "      <td>0.400547</td>\n",
       "      <td>54413</td>\n",
       "      <td>9599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>14184</td>\n",
       "      <td>0.384729</td>\n",
       "      <td>7260</td>\n",
       "      <td>1467</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Total tokens  Linguistic Quality Ratio  stopwords  not in wordnet  \\\n",
       "anger            52177                  0.390057      26879            4946   \n",
       "fear             44498                  0.398782      22763            3990   \n",
       "joy             131401                  0.401565      67405           11230   \n",
       "love             33833                  0.389590      17521            3131   \n",
       "sadness         106784                  0.400547      54413            9599   \n",
       "surprise         14184                  0.384729       7260            1467   \n",
       "\n",
       "          symbols/numbers/etc  \n",
       "anger                       0  \n",
       "fear                        0  \n",
       "joy                         0  \n",
       "love                        0  \n",
       "sadness                     0  \n",
       "surprise                    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# function to calculate linguistic quality ratio for a given text\n",
    "def calculate_linguistic_quality(tweets):\n",
    "    tokens = []\n",
    "    for sentence in tweets:\n",
    "        tokens.extend(word_tokenize(sentence))\n",
    "    \n",
    "    # Count the total number of tokens\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    # Initialize counters for stopwords, words not in WordNet, symbols, links, and numerals\n",
    "    stopwords_count = 0\n",
    "    not_in_wordnet_count = 0\n",
    "    symbols_count = 0\n",
    "    links_count = 0\n",
    "    numerals_count = 0\n",
    "\n",
    "    # Define the set of English stopwords\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    # Define the set of English words in WordNet\n",
    "    english_words = set(words.words())\n",
    "\n",
    "    # check each token for\n",
    "    for token in tokens:\n",
    "        # stopwords\n",
    "        if token in english_stopwords:\n",
    "            stopwords_count += 1\n",
    "        # not in WordNet\n",
    "        if token not in english_words:\n",
    "            not_in_wordnet_count += 1\n",
    "        # symbols\n",
    "        if not token.isalnum():\n",
    "            symbols_count += 1\n",
    "        # links\n",
    "        if token.startswith(\"http://\") or token.startswith(\"https://\"):\n",
    "            links_count += 1\n",
    "        # numerals\n",
    "        if token.isnumeric():\n",
    "            numerals_count += 1\n",
    "\n",
    "    # calculate linguistic quality ratio\n",
    "    cleaned_tokens = total_tokens - stopwords_count - not_in_wordnet_count - symbols_count - links_count - numerals_count\n",
    "    linguistic_quality_ratio = cleaned_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "    return total_tokens, linguistic_quality_ratio, stopwords_count, not_in_wordnet_count, symbols_count + links_count + numerals_count\n",
    "\n",
    "# calculate linguistic quality ratio for each category\n",
    "linguistic_quality_ratios = {}\n",
    "for category, tweets in dataframe_categories.items():\n",
    "    linguistic_quality_ratios[category] = calculate_linguistic_quality(tweets)\n",
    "\n",
    "\n",
    "display(pd.DataFrame.from_dict(linguistic_quality_ratios, orient='index', columns=['Total tokens', 'Linguistic Quality Ratio', 'stopwords', 'not in wordnet','symbols/numbers/etc']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
