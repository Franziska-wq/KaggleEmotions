{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fee3c3",
   "metadata": {},
   "source": [
    "# Exc 2\n",
    "\n",
    "Inspired from\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018d8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions_and_variables as fs\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('words')\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d3e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes [key: sentences] for test were successfully created and stored as .csv\n",
      "Dataframes [key: sentences] for val were successfully created and stored as .csv\n",
      "Dataframes [key: sentences] for train were successfully created and stored as .csv\n",
      "Dataframes [key: sentences] for complete were successfully created and stored as .csv\n"
     ]
    }
   ],
   "source": [
    "def concat_categories_to_file(file_path, file_name, export_path):\n",
    "    data = pd.read_csv(\"{}/{}.txt\".format(file_path,file_name),sep=';')\n",
    "    tweets = data.iloc[:, 0]\n",
    "    categories = data.iloc[:, 1]\n",
    "    \n",
    "    category_tweets = {}\n",
    "    for category, tweet in zip(categories, tweets):\n",
    "        if category not in category_tweets:\n",
    "            category_tweets[category] = []\n",
    "        category_tweets[category].append(tweet)\n",
    "\n",
    "    result = pd.DataFrame(category_tweets.items(),columns=['Category', 'Concatenated_Tweets'])\n",
    "    result.to_csv('{}/{}.csv'.format(export_path,file_name), index=False, encoding='utf-8')\n",
    "    print('Dataframes [key: sentences] for {} were successfully created and stored as .csv'.format(file_name))\n",
    "    return category_tweets\n",
    "\n",
    "for file in ['test','val','train','complete']:\n",
    "    if file == 'complete':\n",
    "        dataframe_categories = concat_categories_to_file('data',file,'categories')\n",
    "    else:\n",
    "        concat_categories_to_file('data',file,'categories')\n",
    "\n",
    "dataframe_categories = dict(sorted(dataframe_categories.items()))\n",
    "categories = dataframe_categories.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3424340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Stemming and remove Stopwords\n",
    "def preprocess(sentences, append):\n",
    "    cleaned = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if not type(sentences) == list: \n",
    "            sentence = sentences\n",
    "            break_p = True\n",
    "        stop_words = list(set(stopwords.words('english')))\n",
    "        stop_words.extend(['im', 'ive','dont','cant'])\n",
    "        stemmer = PorterStemmer()\n",
    "    \n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        cleaned_words = [stemmer.stem(w) for w in word_tokens if not w.lower() in stop_words]\n",
    "        if append:\n",
    "            cleaned.append(cleaned_words)\n",
    "        else:\n",
    "            cleaned.extend(cleaned_words)\n",
    "        if break_p: \n",
    "            break\n",
    "    return cleaned\n",
    "\n",
    "# Returns the words of a sentence\n",
    "def sent_to_words(sentences: list):\n",
    "        return [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81330511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lda(category: str, data: list, shoud_preprocess: bool, num_topis: int, num_words: int):\n",
    "    \n",
    "    data_words = []\n",
    "\n",
    "    if not shoud_preprocess:\n",
    "        data_words = list(sent_to_words(data))\n",
    "\n",
    "    else:\n",
    "        data_words.append(preprocess(data))\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    \n",
    "    # Create Corpus\n",
    "    texts = data_words\n",
    "    \n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=id2word,\n",
    "                                           num_topics=num_topics)\n",
    "\n",
    "    # Print the Keyword with the specified number of words\n",
    "    print(category + \":\")\n",
    "    keywords = lda_model.print_topics(num_words=num_words)\n",
    "    pprint(keywords)\n",
    "    file_path = fs.result_path + 'lda/lda_ana_' + category + '.csv'\n",
    "    fs.write_to_file(file_path=file_path, \n",
    "                     content=str(keywords), new=True)\n",
    "    print('File for \"' + category +'\" successfully created at: ' + file_path)\n",
    "    return id2word, corpus, lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c810a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger:\n",
      "[(0, '0.071*\"i\" + 0.028*\"to\" + 0.027*\"feel\" + 0.027*\"the\"'),\n",
      " (1, '0.095*\"i\" + 0.041*\"feel\" + 0.032*\"and\" + 0.020*\"to\"'),\n",
      " (2, '0.069*\"i\" + 0.029*\"the\" + 0.028*\"and\" + 0.027*\"to\"')]\n",
      "File for \"anger\" successfully created at: ./results/lda/lda_ana_anger.csv\n",
      "fear:\n",
      "[(0, '0.075*\"i\" + 0.034*\"to\" + 0.033*\"and\" + 0.031*\"feel\"'),\n",
      " (1, '0.078*\"i\" + 0.034*\"feel\" + 0.029*\"the\" + 0.024*\"a\"'),\n",
      " (2, '0.076*\"i\" + 0.029*\"to\" + 0.029*\"and\" + 0.025*\"feel\"')]\n",
      "File for \"fear\" successfully created at: ./results/lda/lda_ana_fear.csv\n",
      "joy:\n",
      "[(0, '0.074*\"i\" + 0.039*\"the\" + 0.035*\"feel\" + 0.034*\"to\"'),\n",
      " (1, '0.076*\"i\" + 0.035*\"feel\" + 0.035*\"and\" + 0.023*\"feeling\"'),\n",
      " (2, '0.077*\"i\" + 0.035*\"feel\" + 0.032*\"to\" + 0.030*\"and\"')]\n",
      "File for \"joy\" successfully created at: ./results/lda/lda_ana_joy.csv\n",
      "love:\n",
      "[(0, '0.053*\"i\" + 0.031*\"the\" + 0.030*\"feel\" + 0.029*\"and\"'),\n",
      " (1, '0.084*\"i\" + 0.037*\"feel\" + 0.027*\"and\" + 0.025*\"to\"'),\n",
      " (2, '0.061*\"i\" + 0.033*\"and\" + 0.032*\"to\" + 0.026*\"the\"')]\n",
      "File for \"love\" successfully created at: ./results/lda/lda_ana_love.csv\n",
      "sadness:\n",
      "[(0, '0.065*\"i\" + 0.042*\"feel\" + 0.033*\"and\" + 0.029*\"to\"'),\n",
      " (1, '0.094*\"i\" + 0.035*\"feel\" + 0.031*\"the\" + 0.027*\"to\"'),\n",
      " (2, '0.065*\"i\" + 0.041*\"and\" + 0.033*\"feel\" + 0.020*\"to\"')]\n",
      "File for \"sadness\" successfully created at: ./results/lda/lda_ana_sadness.csv\n",
      "surprise:\n",
      "[(0, '0.061*\"i\" + 0.031*\"feel\" + 0.031*\"the\" + 0.029*\"to\"'),\n",
      " (1, '0.065*\"i\" + 0.028*\"feel\" + 0.025*\"and\" + 0.017*\"to\"'),\n",
      " (2, '0.079*\"i\" + 0.032*\"and\" + 0.029*\"the\" + 0.025*\"feel\"')]\n",
      "File for \"surprise\" successfully created at: ./results/lda/lda_ana_surprise.csv\n"
     ]
    }
   ],
   "source": [
    "num_topics = 3\n",
    "num_words = 4\n",
    "lda_dict = dict.fromkeys(dataframe_categories.keys(), [])\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "index = 0\n",
    "for category in categories:    \n",
    "    # Perform LDA for the current topic\n",
    "    lda_dict[category] = perform_lda(category=category, \n",
    "                                             data=dataframe_categories[category], shoud_preprocess=False, \n",
    "                                             num_topis=num_topics, num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b055d",
   "metadata": {},
   "source": [
    "### Extra function and creation of new cleaned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba4bc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for \"test\" successfully created at: categories\n",
      "File for \"val\" successfully created at: categories\n",
      "File for \"train\" successfully created at: categories\n",
      "File for \"complete\" successfully created at: categories\n"
     ]
    }
   ],
   "source": [
    "def write_cleaned_csv(file_path, file_name, export_path):\n",
    "    file = fs.read_file('{}/{}.txt'.format(file_path,file_name),';')\n",
    "    cleaned_dataframe = {key:[] for key in categories}\n",
    "    for line in file[:5]:\n",
    "        cleaned_dataframe[line[1]].append(' '.join(preprocess(line[0], False)))\n",
    "\n",
    "    result = pd.DataFrame(cleaned_dataframe.items(),columns=['Category', 'Concatenated_Tweets'])\n",
    "    result.to_csv('{}/{}_cleaned.csv'.format(export_path,file_name), index=False, encoding='utf-8')\n",
    "    print('File for \"' + file_name +'\" successfully created at: ' + export_path)\n",
    "    \n",
    "for file in ['test','val','train','complete']:\n",
    "    write_cleaned_csv('data',file,'categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5bb40",
   "metadata": {},
   "source": [
    "# Exc 3\n",
    "\n",
    "Inspired from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b5cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "from scipy.spatial import distance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d495223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pyLDAvis to visualize LDA\n",
    "def visualize_lda(category: str, id2word, corpus, lda_model, num_topics: int):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(fs.result_path + 'lda/', exist_ok=True)\n",
    "    \n",
    "    filename = fs.result_path + 'lda/ldavis_prepared_' + category + '_' +  str(num_topics)\n",
    "    LDAvis_data_filepath = os.path.join(filename)\n",
    "    \n",
    "    ### this is a bit time consuming - make the if statement True\n",
    "    ### if you want to execute visualization prep yourself\n",
    "    if 1 == 1:\n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, n_jobs=1)\n",
    "        \n",
    "        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "            pickle.dump(LDAvis_prepared, f)\n",
    "    \n",
    "    # load the pre-prepared pyLDAvis data from disk\n",
    "    with open(LDAvis_data_filepath, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "    html_filename = filename + '.html'\n",
    "    pyLDAvis.save_html(LDAvis_prepared, html_filename)\n",
    "    print('File for \"' + category + '\" successfully created at: ' + html_filename)\n",
    "    return LDAvis_prepared, html_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29c4f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for \"anger\" successfully created at: ./results/lda/ldavis_prepared_anger_3.html\n",
      "File for \"fear\" successfully created at: ./results/lda/ldavis_prepared_fear_3.html\n",
      "File for \"joy\" successfully created at: ./results/lda/ldavis_prepared_joy_3.html\n",
      "File for \"love\" successfully created at: ./results/lda/ldavis_prepared_love_3.html\n",
      "File for \"sadness\" successfully created at: ./results/lda/ldavis_prepared_sadness_3.html\n",
      "File for \"surprise\" successfully created at: ./results/lda/ldavis_prepared_surprise_3.html\n"
     ]
    }
   ],
   "source": [
    "for category in categories:    \n",
    "    # Create visualisation\n",
    "    prepared_data, html_filename = visualize_lda(category=category, id2word=lda_dict[category][0], \n",
    "                                                 corpus=lda_dict[category][1], \n",
    "                                                 lda_model=lda_dict[category][2], \n",
    "                                                 num_topics=num_topics)\n",
    "    \n",
    "    # Display the visualization directly in the notebook\n",
    "    #display(pyLDAvis.display(prepared_data))\n",
    "    \n",
    "    # Open the HTML files in a web browser (if running on windows: replace \"open \" with \"start \")\n",
    "    #os.system(\"open \" + html_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2e2376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_keywords(keywords):\n",
    "    words = []\n",
    "    for word in keywords:\n",
    "        word = ''.join(char for char in word if char.isalpha())\n",
    "        words.append(str(word))\n",
    "    return words\n",
    "\n",
    "def calculate_closeness(topic1, topic2, model, vector_size):\n",
    "    topic1_words = clean_keywords(topic1.split('+'))\n",
    "    topic2_words = clean_keywords(topic2.split('+'))\n",
    "    \n",
    "    # I1\n",
    "    measure1 = 0;\n",
    "    for word in topic1_words:\n",
    "        if word in topic2_words: \n",
    "            measure1 += 2\n",
    "    \n",
    "    #measure1 = points/0.5*len(topic1 + topic2) # get percentage\n",
    "    \n",
    "    # I2\n",
    "    \n",
    "    # calculate avg. word2vec embeddings per topic\n",
    "    avg_topic_vector1 = np.empty(vector_size)\n",
    "    for word in topic1_words:\n",
    "        avg_topic_vector1 += model.wv[word]\n",
    "    avg_topic_vector1 /= len(topic1_words)\n",
    "    \n",
    "    avg_topic_vector2 = np.empty(vector_size)\n",
    "    for word in topic2_words:\n",
    "        avg_topic_vector2 += model.wv[word]\n",
    "    avg_topic_vector2 /= len(topic2_words)\n",
    "    \n",
    "    measure2 = 1 - distance.cosine(avg_topic_vector1, avg_topic_vector2)\n",
    "    \n",
    "    return measure1, measure2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef8533f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>topics</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fear</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fear</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>joy</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>joy</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>joy</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.986420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>love</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>love</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>love</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sadness</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sadness</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.999831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sadness</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>surprise</td>\n",
       "      <td>T0&amp;T1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.996157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>surprise</td>\n",
       "      <td>T0&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>surprise</td>\n",
       "      <td>T1&amp;T2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category topics  I1        I2\n",
       "0      anger  T0&T1   6  0.999999\n",
       "1      anger  T0&T2   6  0.999996\n",
       "2      anger  T1&T2   6  0.999892\n",
       "3       fear  T0&T1   4  0.999996\n",
       "4       fear  T0&T2   8  0.999487\n",
       "5       fear  T1&T2   4  0.999258\n",
       "6        joy  T0&T1   4  0.999668\n",
       "7        joy  T0&T2   6  0.999864\n",
       "8        joy  T1&T2   6  0.986420\n",
       "9       love  T0&T1   6  0.999924\n",
       "10      love  T0&T2   6  0.999868\n",
       "11      love  T1&T2   6  0.999811\n",
       "12   sadness  T0&T1   6  0.999133\n",
       "13   sadness  T0&T2   8  0.999831\n",
       "14   sadness  T1&T2   6  0.999826\n",
       "15  surprise  T0&T1   6  0.996157\n",
       "16  surprise  T0&T2   6  0.999745\n",
       "17  surprise  T1&T2   6  0.999859"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "closeness_data = pd.DataFrame(columns=['category','topics','I1','I2']) # dataframe to store values in\n",
    "\n",
    "index = 0 # index to iterate through dataframe\n",
    "vector_size = 1000 # vector size for model\n",
    "for category in categories:\n",
    "    # get lda_resut for the current category and transfer it to tuples\n",
    "    with open('./results/lda/lda_ana_' + category + '.csv', 'r') as file:\n",
    "        lda_data = file.readline()\n",
    "    topics = ast.literal_eval(lda_data)\n",
    "    \n",
    "    # get training data for model and transform it to a list of words per sentence\n",
    "    model_data = []\n",
    "    for sentence in dataframe_categories[category]:\n",
    "        model_data.append(sentence.split(' '))\n",
    "        \n",
    "    # train model\n",
    "    model = gensim.models.Word2Vec(sentences=model_data, min_count=1, \n",
    "                              vector_size=vector_size, window=5)\n",
    "    \n",
    "    # calculate closeness of topics\n",
    "    for i in range(len(topics)):\n",
    "        for j in range(i + 1, len(topics)):\n",
    "            closeness = calculate_closeness(topic1=topics[i][1], topic2=topics[j][1], \n",
    "                                            model=model, vector_size=vector_size)\n",
    "            closeness_data.loc[index] = category, f\"T{i}&T{j}\", closeness[0], closeness[1]\n",
    "            index += 1\n",
    "display(closeness_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e706bc",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c65e213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate linguistic quality ratio for a given text\n",
    "def calculate_linguistic_quality(text_data):\n",
    "    tokens = []\n",
    "    for sentence in text_data:\n",
    "        tokens.extend(word_tokenize(sentence))\n",
    "    tokens = list(set(tokens))\n",
    "    \n",
    "    # Count the total number of tokens\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    # Initialize counters for stopwords, words not in WordNet, symbols, links, and numerals\n",
    "    stopwords_count = 0\n",
    "    not_in_wordnet_count = 0\n",
    "    symbols_count = 0\n",
    "    links_count = 0\n",
    "    numerals_count = 0\n",
    "\n",
    "    # Define the set of English stopwords\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    # Define the set of English words in WordNet\n",
    "    english_words = set(words.words())\n",
    "\n",
    "    # Check each token\n",
    "    for token in tokens:\n",
    "        # Check for stopwords\n",
    "        if token in english_stopwords:\n",
    "            stopwords_count += 1\n",
    "        # Check if the token is not in WordNet\n",
    "        if token not in english_words:\n",
    "            not_in_wordnet_count += 1\n",
    "        # Check for symbols (you can define your own criteria for symbols)\n",
    "        if not token.isalnum():\n",
    "            symbols_count += 1\n",
    "        # Check for links\n",
    "        if token.startswith(\"http://\") or token.startswith(\"https://\"):\n",
    "            links_count += 1\n",
    "        # Check for numerals\n",
    "        if token.isnumeric():\n",
    "            numerals_count += 1\n",
    "\n",
    "    # Calculate the linguistic quality ratio\n",
    "    cleaned_tokens = total_tokens - stopwords_count - not_in_wordnet_count - symbols_count - links_count - numerals_count\n",
    "    linguistic_quality_ratio = cleaned_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "    return linguistic_quality_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7215b802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>Linguistic Quality Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anger</td>\n",
       "      <td>0.647572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>0.659458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>joy</td>\n",
       "      <td>0.604459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>0.649909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadness</td>\n",
       "      <td>0.628194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>surprise</td>\n",
       "      <td>0.673271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  Linguistic Quality Ratio\n",
       "0     anger                  0.647572\n",
       "1      fear                  0.659458\n",
       "2       joy                  0.604459\n",
       "3      love                  0.649909\n",
       "4   sadness                  0.628194\n",
       "5  surprise                  0.673271"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate linguistic quality ratio for each category\n",
    "linguistic_quality_ratios = {}\n",
    "for category, text_data in dataframe_categories.items():\n",
    "    linguistic_quality_ratios[category] = calculate_linguistic_quality(text_data)\n",
    "\n",
    "display(pd.DataFrame(list(linguistic_quality_ratios.items()), columns=['category', 'Linguistic Quality Ratio']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
